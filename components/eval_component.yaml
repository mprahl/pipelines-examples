# PIPELINE DEFINITION
# Name: evaluate-model
# Inputs:
#    add_bos_token: bool [Default: True]
#    batch_size: int [Default: 1.0]
#    custom_translation_dataset: str
#    dtype: str [Default: 'bfloat16']
#    gpu_memory_utilization: float [Default: 0.8]
#    include_classification_tasks: bool [Default: True]
#    include_summarization_tasks: bool [Default: True]
#    limit: int
#    max_batch_size: int
#    max_model_len: int [Default: 4096.0]
#    model_path: str
#    verbosity: str [Default: 'INFO']
# Outputs:
#    output_metrics: system.Metrics
#    output_results: system.Artifact
components:
  comp-evaluate-model:
    executorLabel: exec-evaluate-model
    inputDefinitions:
      parameters:
        add_bos_token:
          defaultValue: true
          isOptional: true
          parameterType: BOOLEAN
        batch_size:
          defaultValue: 1.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        custom_translation_dataset:
          isOptional: true
          parameterType: STRING
        dtype:
          defaultValue: bfloat16
          isOptional: true
          parameterType: STRING
        gpu_memory_utilization:
          defaultValue: 0.8
          isOptional: true
          parameterType: NUMBER_DOUBLE
        include_classification_tasks:
          defaultValue: true
          isOptional: true
          parameterType: BOOLEAN
        include_summarization_tasks:
          defaultValue: true
          isOptional: true
          parameterType: BOOLEAN
        limit:
          isOptional: true
          parameterType: NUMBER_INTEGER
        max_batch_size:
          isOptional: true
          parameterType: NUMBER_INTEGER
        max_model_len:
          defaultValue: 4096.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        model_path:
          parameterType: STRING
        verbosity:
          defaultValue: INFO
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        output_metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        output_results:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-evaluate-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - evaluate_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'transformers'\
          \ 'torch' 'accelerate' 'lm-eval[vllm]' 'unitxt' 'sacrebleu' 'datasets' \
          \ &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef evaluate_model(\n    model_path: str,\n    output_metrics: dsl.Output[dsl.Metrics],\n\
          \    output_results: dsl.Output[dsl.Artifact],\n    batch_size: int = 1,\n\
          \    limit: int = None,\n    max_model_len: int = 4096,\n    gpu_memory_utilization:\
          \ float = 0.8,\n    dtype: str = \"bfloat16\",\n    add_bos_token: bool\
          \ = True,\n    include_classification_tasks: bool = True,\n    include_summarization_tasks:\
          \ bool = True,\n    custom_translation_dataset: str = None,\n    verbosity:\
          \ str = \"INFO\",\n    max_batch_size: int = None,\n):\n    import logging\n\
          \    import os\n    import json\n    import time\n    import random\n  \
          \  from typing import Dict, Any, Optional\n\n    from lm_eval.tasks.unitxt\
          \ import task\n    from lm_eval.api.registry import get_model\n    from\
          \ lm_eval.api.model import LM\n    from lm_eval.evaluator import evaluate\n\
          \    from lm_eval.tasks import get_task_dict\n    from lm_eval.api.instance\
          \ import Instance\n    from lm_eval import tasks\n    from lm_eval.api.task\
          \ import TaskConfig\n    from lm_eval.api.metrics import mean\n    from\
          \ datasets import load_from_disk\n    import torch\n    import sacrebleu\n\
          \n    class TranslationTask(tasks.Task):\n        \"\"\"\n        A custom\
          \ lm-eval task for translation, using the greedy_until method\n        and\
          \ evaluating with the BLEU metric.\n        \"\"\"\n\n        VERSION =\
          \ 0\n\n        def __init__(self, dataset_path, task_name: str):\n     \
          \       self.dataset_path = dataset_path\n            self.task_name = task_name\n\
          \            config = TaskConfig(task=task_name, dataset_path=dataset_path)\n\
          \            super().__init__(config=config)\n            self.config.task\
          \ = task_name\n            self.fewshot_rnd = random.Random()\n\n      \
          \  def download(\n            self, data_dir=None, cache_dir=None, download_mode=None,\
          \ **kwargs\n        ) -> None:\n            self.dataset = {\"test\": load_from_disk(self.dataset_path)}\n\
          \n        def has_test_docs(self):\n            return \"test\" in self.dataset\n\
          \n        def has_validation_docs(self):\n            return False\n\n \
          \       def has_training_docs(self):\n            return False\n\n     \
          \   def test_docs(self):\n            return self.dataset[\"test\"]\n\n\
          \        def doc_to_text(self, doc):\n            return doc[\"prompt\"\
          ]\n\n        def doc_to_target(self, doc):\n            return doc[\"completion\"\
          ]\n\n        def construct_requests(self, doc, ctx, **kwargs):\n       \
          \     kwargs.pop(\"apply_chat_template\", False)\n            kwargs.pop(\"\
          chat_template\", False)\n            return Instance(\n                request_type=\"\
          generate_until\",\n                doc=doc,\n                arguments=(ctx,\
          \ {}),\n                idx=0,\n                **kwargs,\n            )\n\
          \n        def process_results(self, doc, results):\n            (generated_text,)\
          \ = results\n\n            prediction = generated_text.strip()\n\n     \
          \       predictions = [prediction]\n            references = [[self.doc_to_target(doc).strip()]]\n\
          \n            bleu_score = sacrebleu.corpus_bleu(predictions, references).score\n\
          \n            exact_match = 1.0 if prediction == references[0][0] else 0.0\n\
          \n            return {\"bleu\": bleu_score, \"exact_match\": exact_match}\n\
          \n        def aggregation(self):\n            return {\"bleu\": mean, \"\
          exact_match\": mean}\n\n        def should_decontaminate(self):\n      \
          \      return False\n\n        def doc_to_prefix(self, doc):\n         \
          \   return \"\"\n\n        def higher_is_better(self):\n            return\
          \ {\"bleu\": True, \"exact_match\": True}\n\n    TASK_CONFIGS = {\n    \
          \    \"classification\": [\n            {\n                \"task\": \"\
          classification_rte_simple\",\n                \"recipe\": \"card=cards.rte,template=templates.classification.multi_class.relation.simple\"\
          ,\n                \"group\": \"classification\",\n                \"output_type\"\
          : \"generate_until\",\n            },\n            {\n                \"\
          task\": \"classification_rte_default\",\n                \"recipe\": \"\
          card=cards.rte,template=templates.classification.multi_class.relation.default\"\
          ,\n                \"group\": \"classification\",\n                \"output_type\"\
          : \"generate_until\",\n            },\n            {\n                \"\
          task\": \"classification_rte_wnli\",\n                \"recipe\": \"card=cards.wnli,template=templates.classification.multi_class.relation.simple\"\
          ,\n                \"group\": \"classification\",\n                \"output_type\"\
          : \"generate_until\",\n            },\n        ],\n        \"summarization\"\
          : [\n            {\n                \"task\": \"summarization_xsum_formal\"\
          ,\n                \"recipe\": \"card=cards.xsum,template=templates.summarization.abstractive.formal,num_demos=0\"\
          ,\n                \"group\": \"summarization\",\n                \"output_type\"\
          : \"generate_until\",\n            }\n        ],\n    }\n\n    logging.basicConfig(\n\
          \        level=getattr(logging, verbosity.upper()),\n        format=\"%(asctime)s\
          \ - %(name)s - %(levelname)s - %(message)s\",\n    )\n    logger = logging.getLogger(__name__)\n\
          \n    os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n    os.environ[\"\
          HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n\n    logger.info(\"Validating\
          \ parameters...\")\n\n    if not torch.cuda.is_available():\n        raise\
          \ ValueError(\"CUDA is not available\")\n\n    if not (0.0 <= gpu_memory_utilization\
          \ <= 1.0):\n        raise ValueError(\"gpu_memory_utilization must be between\
          \ 0.0 and 1.0\")\n\n    if batch_size <= 0:\n        raise ValueError(\"\
          batch_size must be positive\")\n\n    if max_model_len <= 0:\n        raise\
          \ ValueError(\"max_model_len must be positive\")\n\n    if limit is not\
          \ None and limit <= 0:\n        raise ValueError(\"limit must be positive\
          \ or None\")\n\n    if (\n        not include_classification_tasks\n   \
          \     and not include_summarization_tasks\n        and not custom_translation_dataset\n\
          \    ):\n        raise ValueError(\n            \"At least one of include_classification_tasks,\
          \ include_summarization_tasks, or custom_translation_dataset must be provided\"\
          \n        )\n\n    logger.info(\"Parameter validation passed\")\n\n    logger.info(\"\
          Creating tasks...\")\n    start_time = time.time()\n\n    eval_tasks = []\n\
          \n    if custom_translation_dataset:\n        logger.info(\"Adding custom\
          \ translation task...\")\n        translation_task = TranslationTask(\n\
          \            custom_translation_dataset, \"custom_translation\"\n      \
          \  )\n        eval_tasks.append(translation_task)\n\n    if include_classification_tasks:\n\
          \        logger.info(\"Adding classification tasks...\")\n        classification_configs\
          \ = TASK_CONFIGS[\"classification\"]\n\n        for config in classification_configs:\n\
          \            task_obj = task.Unitxt(config=config)\n            # TODO:\
          \ Remove after https://github.com/EleutherAI/lm-evaluation-harness/pull/3225\
          \ is merged.\n            task_obj.config.task = config[\"task\"]\n    \
          \        eval_tasks.append(task_obj)\n\n    if include_summarization_tasks:\n\
          \        logger.info(\"Adding summarization tasks...\")\n        summarization_config\
          \ = TASK_CONFIGS[\"summarization\"][0]\n\n        task_obj = task.Unitxt(config=summarization_config)\n\
          \        # TODO: Remove after https://github.com/EleutherAI/lm-evaluation-harness/pull/3225\
          \ is merged.\n        task_obj.config.task = summarization_config[\"task\"\
          ]\n        eval_tasks.append(task_obj)\n\n    task_dict = get_task_dict(eval_tasks)\n\
          \    logger.info(f\"Created {len(eval_tasks)} tasks in {time.time() - start_time:.2f}s\"\
          )\n\n    logger.info(\"Loading model...\")\n    start_time = time.time()\n\
          \n    try:\n        model_args = {\n            \"add_bos_token\": add_bos_token,\n\
          \            \"dtype\": dtype,\n            \"max_model_len\": max_model_len,\n\
          \            \"gpu_memory_utilization\": gpu_memory_utilization,\n     \
          \       \"pretrained\": model_path,\n        }\n\n        model_class =\
          \ get_model(\"vllm\")\n        additional_config = {\n            \"batch_size\"\
          : batch_size,\n            \"max_batch_size\": max_batch_size,\n       \
          \     \"device\": None,\n        }\n\n        loaded_model = model_class.create_from_arg_obj(model_args,\
          \ additional_config)\n        logger.info(f\"Model loaded successfully in\
          \ {time.time() - start_time:.2f}s\")\n    except Exception as e:\n     \
          \   logger.error(f\"Failed to load model: {e}\")\n        raise RuntimeError(f\"\
          Model loading failed: {e}\")\n\n    logger.info(\"Starting evaluation...\"\
          )\n    start_time = time.time()\n\n    results = evaluate(\n        lm=loaded_model,\n\
          \        task_dict=task_dict,\n        limit=limit,\n        verbosity=verbosity,\n\
          \    )\n\n    logger.info(f\"Evaluation completed in {time.time() - start_time:.2f}s\"\
          )\n\n    logger.info(\"Saving results...\")\n\n    def clean_for_json(obj):\n\
          \        \"\"\"Recursively clean objects to make them JSON serializable.\"\
          \"\"\n        if isinstance(obj, dict):\n            return {k: clean_for_json(v)\
          \ for k, v in obj.items()}\n        elif isinstance(obj, list):\n      \
          \      return [clean_for_json(item) for item in obj]\n        elif isinstance(obj,\
          \ (int, float, str, bool, type(None))):\n            return obj\n      \
          \  else:\n            # Convert non-serializable objects to string representation\n\
          \            return str(obj)\n\n    clean_results = clean_for_json(results)\n\
          \n    output_results.name = \"results.json\"\n\n    with open(output_results.path,\
          \ \"w\") as f:\n        json.dump(clean_results, f, indent=2)\n    logger.info(f\"\
          Results saved to {output_results.path}\")\n\n    logger.info(\"Logging metrics...\"\
          )\n\n    for task_name, task_results in clean_results[\"results\"].items():\n\
          \        for metric_name, metric_value in task_results.items():\n      \
          \      if isinstance(metric_value, (int, float)):\n                # Skip\
          \ metrics that are 0 due to a bug in the RHOAI UI.\n                # TODO:\
          \ Fix RHOAI UI to handle 0 values.\n                # TODO: Ignore store_session_info\
          \ from metrics in RHOAI UI.\n                if metric_value == 0:\n   \
          \                 continue\n\n                metric_key = f\"{task_name}_{metric_name}\"\
          \n                output_metrics.log_metric(metric_key, metric_value)\n\
          \                logger.debug(f\"Logged metric: {metric_key} = {metric_value}\"\
          )\n\n    logger.info(\"Metrics logged successfully\")\n\n    logger.info(\"\
          Pipeline completed successfully\")\n\n"
        image: registry.access.redhat.com/ubi9/python-311:latest
pipelineInfo:
  name: evaluate-model
root:
  dag:
    outputs:
      artifacts:
        output_metrics:
          artifactSelectors:
          - outputArtifactKey: output_metrics
            producerSubtask: evaluate-model
        output_results:
          artifactSelectors:
          - outputArtifactKey: output_results
            producerSubtask: evaluate-model
    tasks:
      evaluate-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-evaluate-model
        inputs:
          parameters:
            add_bos_token:
              componentInputParameter: add_bos_token
            batch_size:
              componentInputParameter: batch_size
            custom_translation_dataset:
              componentInputParameter: custom_translation_dataset
            dtype:
              componentInputParameter: dtype
            gpu_memory_utilization:
              componentInputParameter: gpu_memory_utilization
            include_classification_tasks:
              componentInputParameter: include_classification_tasks
            include_summarization_tasks:
              componentInputParameter: include_summarization_tasks
            limit:
              componentInputParameter: limit
            max_batch_size:
              componentInputParameter: max_batch_size
            max_model_len:
              componentInputParameter: max_model_len
            model_path:
              componentInputParameter: model_path
            verbosity:
              componentInputParameter: verbosity
        taskInfo:
          name: evaluate-model
  inputDefinitions:
    parameters:
      add_bos_token:
        defaultValue: true
        isOptional: true
        parameterType: BOOLEAN
      batch_size:
        defaultValue: 1.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      custom_translation_dataset:
        isOptional: true
        parameterType: STRING
      dtype:
        defaultValue: bfloat16
        isOptional: true
        parameterType: STRING
      gpu_memory_utilization:
        defaultValue: 0.8
        isOptional: true
        parameterType: NUMBER_DOUBLE
      include_classification_tasks:
        defaultValue: true
        isOptional: true
        parameterType: BOOLEAN
      include_summarization_tasks:
        defaultValue: true
        isOptional: true
        parameterType: BOOLEAN
      limit:
        isOptional: true
        parameterType: NUMBER_INTEGER
      max_batch_size:
        isOptional: true
        parameterType: NUMBER_INTEGER
      max_model_len:
        defaultValue: 4096.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      model_path:
        parameterType: STRING
      verbosity:
        defaultValue: INFO
        isOptional: true
        parameterType: STRING
  outputDefinitions:
    artifacts:
      output_metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      output_results:
        artifactType:
          schemaTitle: system.Artifact
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.14.1
