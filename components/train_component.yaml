# PIPELINE DEFINITION
# Name: train-model
# Description: Train a large language model using distributed training with LoRA fine-tuning.
#              This function creates and manages a Kubernetes TrainJob for distributed training
#              of a large language model using LoRA (Low-Rank Adaptation) fine-tuning. It handles
#              the complete training workflow including job creation, monitoring, and artifact
#              collection.
# Inputs:
#    adam_beta1: float [Default: 0.9]
#    adam_beta2: float [Default: 0.999]
#    adam_epsilon: float [Default: 1e-08]
#    batch_size: int [Default: 16.0]
#    dataset_path: str
#    epochs: int [Default: 10.0]
#    learning_rate: float [Default: 0.0003]
#    logging_steps: int [Default: 10.0]
#    lora_rank: int [Default: 8.0]
#    max_length: int [Default: 64.0]
#    max_steps: int
#    model_name: str
#    num_nodes: int [Default: 2.0]
#    optimizer: str [Default: 'adamw_torch']
#    pvc_name: str
#    pvc_path: str
#    run_id: str
#    save_merged_model_path: str
#    save_steps: int
#    save_strategy: str [Default: 'epoch']
#    train_node_cpu_request: str [Default: '2']
#    train_node_gpu_request: str [Default: '1']
#    train_node_memory_request: str [Default: '100Gi']
#    trainer_runtime: str [Default: 'torch-distributed']
#    use_flash_attention: bool [Default: False]
#    weight_decay: float [Default: 0.01]
# Outputs:
#    output_metrics: system.Metrics
#    output_model: system.Model
components:
  comp-train-model:
    executorLabel: exec-train-model
    inputDefinitions:
      parameters:
        adam_beta1:
          defaultValue: 0.9
          description: Beta1 parameter for Adam optimizer. Defaults to 0.9.
          isOptional: true
          parameterType: NUMBER_DOUBLE
        adam_beta2:
          defaultValue: 0.999
          description: Beta2 parameter for Adam optimizer. Defaults to 0.999.
          isOptional: true
          parameterType: NUMBER_DOUBLE
        adam_epsilon:
          defaultValue: 1.0e-08
          description: Epsilon parameter for Adam optimizer. Defaults to 1e-8.
          isOptional: true
          parameterType: NUMBER_DOUBLE
        batch_size:
          defaultValue: 16.0
          description: Per-device training batch size. Defaults to 16.
          isOptional: true
          parameterType: NUMBER_INTEGER
        dataset_path:
          description: Path to the training dataset within the PVC.
          parameterType: STRING
        epochs:
          defaultValue: 10.0
          description: Number of training epochs. Defaults to 10.
          isOptional: true
          parameterType: NUMBER_INTEGER
        learning_rate:
          defaultValue: 0.0003
          description: Learning rate for training optimization. Defaults to 3e-4.
          isOptional: true
          parameterType: NUMBER_DOUBLE
        logging_steps:
          defaultValue: 10.0
          description: Number of steps between logging outputs. Defaults to 10.
          isOptional: true
          parameterType: NUMBER_INTEGER
        lora_rank:
          defaultValue: 8.0
          description: LoRA adapter rank (lower = fewer parameters, faster training).
            Defaults to 8.
          isOptional: true
          parameterType: NUMBER_INTEGER
        max_length:
          defaultValue: 64.0
          description: Maximum token sequence length for training. Defaults to 64.
          isOptional: true
          parameterType: NUMBER_INTEGER
        max_steps:
          description: Maximum number of training steps. If specified, overrides epochs.
            Defaults to None.
          isOptional: true
          parameterType: NUMBER_INTEGER
        model_name:
          description: HuggingFace model identifier (e.g., "ibm-granite/granite-3.3-8b-instruct").
          parameterType: STRING
        num_nodes:
          defaultValue: 2.0
          description: Number of nodes for distributed training. Defaults to 2.
          isOptional: true
          parameterType: NUMBER_INTEGER
        optimizer:
          defaultValue: adamw_torch
          description: Optimizer to use (e.g., "adamw_torch", "adamw_torch_fused").
            Defaults to "adamw_torch".
          isOptional: true
          parameterType: STRING
        pvc_name:
          description: Name of the Persistent Volume Claim for data storage that is
            mounted to this component.
          parameterType: STRING
        pvc_path:
          description: Base path within the PVC for storing outputs.
          parameterType: STRING
        run_id:
          description: Unique identifier for this training run. Use dsl.PIPELINE_JOB_ID_PLACEHOLDER.
          parameterType: STRING
        save_merged_model_path:
          description: Path to save the merged model (base + LoRA adapter). Useful
            for saving to the PVC for evaluation. Defaults to None.
          isOptional: true
          parameterType: STRING
        save_steps:
          description: Number of steps between model checkpoints. Defaults to None.
          isOptional: true
          parameterType: NUMBER_INTEGER
        save_strategy:
          defaultValue: epoch
          description: Checkpoint saving strategy ("epoch" or "steps"). Defaults to
            "epoch".
          isOptional: true
          parameterType: STRING
        train_node_cpu_request:
          defaultValue: '2'
          description: CPU request per node (e.g., "2", "4"). Defaults to "2".
          isOptional: true
          parameterType: STRING
        train_node_gpu_request:
          defaultValue: '1'
          description: GPU request per node (e.g., "1", "2"). Defaults to "1".
          isOptional: true
          parameterType: STRING
        train_node_memory_request:
          defaultValue: 100Gi
          description: Memory request per node (e.g., "100Gi", "200Gi"). Defaults
            to "100Gi".
          isOptional: true
          parameterType: STRING
        trainer_runtime:
          defaultValue: torch-distributed
          description: Runtime to use for Kubeflow Trainer. Defaults to "torch-distributed".
          isOptional: true
          parameterType: STRING
        use_flash_attention:
          defaultValue: false
          description: Whether to use Flash Attention 2 for improved performance.
            Defaults to False.
          isOptional: true
          parameterType: BOOLEAN
        weight_decay:
          defaultValue: 0.01
          description: Weight decay for regularization. Defaults to 0.01.
          isOptional: true
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        output_metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        output_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-train-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kubernetes'\
          \  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.1'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_model(\n    model_name: str,\n    pvc_name: str,\n    run_id:\
          \ str,\n    dataset_path: str,\n    pvc_path: str,\n    output_model: dsl.Output[dsl.Model],\n\
          \    output_metrics: dsl.Output[dsl.Metrics],\n    # Training configuration\
          \ parameters\n    epochs: int = 10,\n    lora_rank: int = 8,\n    learning_rate:\
          \ float = 3e-4,\n    batch_size: int = 16,\n    max_length: int = 64,\n\
          \    # Training control parameters\n    max_steps: Optional[int] = None,\n\
          \    logging_steps: int = 10,\n    save_steps: Optional[int] = None,\n \
          \   save_strategy: str = \"epoch\",\n    # Optimizer parameters\n    optimizer:\
          \ str = \"adamw_torch\",\n    adam_beta1: float = 0.9,\n    adam_beta2:\
          \ float = 0.999,\n    adam_epsilon: float = 1e-8,\n    weight_decay: float\
          \ = 0.01,\n    # Performance optimization\n    use_flash_attention: bool\
          \ = False,\n    # Infrastructure parameters\n    num_nodes: int = 2,\n \
          \   train_node_cpu_request: str = \"2\",\n    train_node_gpu_request: str\
          \ = \"1\",\n    train_node_memory_request: str = \"100Gi\",\n    trainer_runtime:\
          \ str = \"torch-distributed\",\n    save_merged_model_path: str = None,\n\
          ):\n    \"\"\"Train a large language model using distributed training with\
          \ LoRA fine-tuning.\n\n    This function creates and manages a Kubernetes\
          \ TrainJob for distributed training\n    of a large language model using\
          \ LoRA (Low-Rank Adaptation) fine-tuning. It handles\n    the complete training\
          \ workflow including job creation, monitoring, and artifact\n    collection.\n\
          \n    Args:\n        model_name (str): HuggingFace model identifier (e.g.,\
          \ \"ibm-granite/granite-3.3-8b-instruct\").\n        pvc_name (str): Name\
          \ of the Persistent Volume Claim for data storage that is mounted to this\
          \ component.\n        run_id (str): Unique identifier for this training\
          \ run. Use dsl.PIPELINE_JOB_ID_PLACEHOLDER.\n        dataset_path (str):\
          \ Path to the training dataset within the PVC.\n        pvc_path (str):\
          \ Base path within the PVC for storing outputs.\n        output_model (dsl.Output[dsl.Model]):\
          \ Kubeflow output artifact for the trained model.\n        output_metrics\
          \ (dsl.Output[dsl.Metrics]): Kubeflow output artifact for training metrics.\n\
          \        epochs (int, optional): Number of training epochs. Defaults to\
          \ 10.\n        lora_rank (int, optional): LoRA adapter rank (lower = fewer\
          \ parameters, faster training). Defaults to 8.\n        learning_rate (float,\
          \ optional): Learning rate for training optimization. Defaults to 3e-4.\n\
          \        batch_size (int, optional): Per-device training batch size. Defaults\
          \ to 16.\n        max_length (int, optional): Maximum token sequence length\
          \ for training. Defaults to 64.\n        max_steps (int, optional): Maximum\
          \ number of training steps. If specified, overrides epochs. Defaults to\
          \ None.\n        logging_steps (int, optional): Number of steps between\
          \ logging outputs. Defaults to 10.\n        save_steps (int, optional):\
          \ Number of steps between model checkpoints. Defaults to None.\n       \
          \ save_strategy (str, optional): Checkpoint saving strategy (\"epoch\" or\
          \ \"steps\"). Defaults to \"epoch\".\n        optimizer (str, optional):\
          \ Optimizer to use (e.g., \"adamw_torch\", \"adamw_torch_fused\"). Defaults\
          \ to \"adamw_torch\".\n        adam_beta1 (float, optional): Beta1 parameter\
          \ for Adam optimizer. Defaults to 0.9.\n        adam_beta2 (float, optional):\
          \ Beta2 parameter for Adam optimizer. Defaults to 0.999.\n        adam_epsilon\
          \ (float, optional): Epsilon parameter for Adam optimizer. Defaults to 1e-8.\n\
          \        weight_decay (float, optional): Weight decay for regularization.\
          \ Defaults to 0.01.\n        use_flash_attention (bool, optional): Whether\
          \ to use Flash Attention 2 for improved performance. Defaults to False.\n\
          \        num_nodes (int, optional): Number of nodes for distributed training.\
          \ Defaults to 2.\n        train_node_cpu_request (str, optional): CPU request\
          \ per node (e.g., \"2\", \"4\"). Defaults to \"2\".\n        train_node_gpu_request\
          \ (str, optional): GPU request per node (e.g., \"1\", \"2\"). Defaults to\
          \ \"1\".\n        train_node_memory_request (str, optional): Memory request\
          \ per node (e.g., \"100Gi\", \"200Gi\"). Defaults to \"100Gi\".\n      \
          \  trainer_runtime (str, optional): Runtime to use for Kubeflow Trainer.\
          \ Defaults to \"torch-distributed\".\n        save_merged_model_path (str,\
          \ optional): Path to save the merged model (base + LoRA adapter). Useful\
          \ for saving to the PVC for evaluation. Defaults to None.\n    \"\"\"\n\
          \    import json\n    import os\n    import shutil\n    import textwrap\n\
          \    import time\n    import inspect\n\n    from kubernetes import client\
          \ as k8s_client, config\n    from kubernetes.client.rest import ApiException\n\
          \n    def get_target_modules(model_name: str) -> list:\n        \"\"\"Get\
          \ appropriate LoRA target modules based on model architecture.\n\n     \
          \   Selects optimal layers for LoRA adaptation based on research findings:\n\
          \        - Attention layers (q_proj, k_proj, v_proj, o_proj) control attention\
          \ patterns\n        - MLP layers (gate_proj, up_proj, down_proj) store task-specific\
          \ knowledge\n\n        Model-specific targeting:\n        - Granite: Attention\
          \ layers only (q,k,v,o)\n        - LLaMA/Mistral/Qwen: Full coverage (attention\
          \ + MLP)\n        - Phi: Uses 'dense' instead of 'o_proj'\n        - Unknown:\
          \ Conservative fallback (q,v)\n\n        Based on LoRA (arXiv:2106.09685),\
          \ QLoRA (arXiv:2305.14314), and model-specific research.\n        \"\"\"\
          \n        model_name_lower = model_name.lower()\n\n        if \"granite\"\
          \ in model_name_lower:\n            return [\"q_proj\", \"v_proj\", \"k_proj\"\
          , \"o_proj\"]\n        elif \"llama\" in model_name_lower:\n           \
          \ return [\n                \"q_proj\",\n                \"v_proj\",\n \
          \               \"k_proj\",\n                \"o_proj\",\n             \
          \   \"gate_proj\",\n                \"up_proj\",\n                \"down_proj\"\
          ,\n            ]\n        elif \"mistral\" in model_name_lower or \"mixtral\"\
          \ in model_name_lower:\n            return [\n                \"q_proj\"\
          ,\n                \"v_proj\",\n                \"k_proj\",\n          \
          \      \"o_proj\",\n                \"gate_proj\",\n                \"up_proj\"\
          ,\n                \"down_proj\",\n            ]\n        elif \"qwen\"\
          \ in model_name_lower:\n            return [\n                \"q_proj\"\
          ,\n                \"v_proj\",\n                \"k_proj\",\n          \
          \      \"o_proj\",\n                \"gate_proj\",\n                \"up_proj\"\
          ,\n                \"down_proj\",\n            ]\n        elif \"phi\" in\
          \ model_name_lower:\n            return [\"q_proj\", \"v_proj\", \"k_proj\"\
          , \"dense\"]\n        else:\n            print(\n                f\"Warning:\
          \ Unknown model architecture for {model_name}, using conservative LoRA targets\"\
          \n            )\n            return [\"q_proj\", \"v_proj\"]\n\n    def\
          \ train_model_func(\n        lora_rank: int,\n        learning_rate: float,\n\
          \        batch_size: int,\n        max_length: int,\n        model_name:\
          \ str,\n        dataset_path: str,\n        epochs: int,\n        pvc_path:\
          \ str,\n        target_modules: list,\n        max_steps: int,\n       \
          \ logging_steps: int,\n        save_steps: int,\n        save_strategy:\
          \ str,\n        optimizer: str,\n        adam_beta1: float,\n        adam_beta2:\
          \ float,\n        adam_epsilon: float,\n        weight_decay: float,\n \
          \       use_flash_attention: bool,\n        save_merged_model_path: str\
          \ = None,\n    ):\n        import os\n        import json\n        import\
          \ torch\n        from datasets import load_from_disk\n        from peft\
          \ import get_peft_model, LoraConfig\n        from transformers import (\n\
          \            AutoModelForCausalLM,\n            AutoTokenizer,\n       \
          \     TrainerCallback,\n        )\n        from trl import SFTConfig, SFTTrainer\n\
          \n        local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n        world_rank\
          \ = int(os.environ.get(\"RANK\", 0))\n        world_size = int(os.environ.get(\"\
          WORLD_SIZE\", 1))\n\n        print(\n            f\"Worker info - Local\
          \ rank: {local_rank}, World rank: {world_rank}, World size: {world_size}\"\
          \n        )\n\n        is_main_worker = world_rank == 0\n\n        class\
          \ MetricsCallback(TrainerCallback):\n            def __init__(self, is_main_worker):\n\
          \                self.is_main_worker = is_main_worker\n                self.initial_loss\
          \ = None\n                self.final_loss = None\n\n            def on_log(self,\
          \ args, state, control, logs=None, **kwargs):\n                if logs and\
          \ self.is_main_worker and \"loss\" in logs:\n                    if self.initial_loss\
          \ is None:\n                        self.initial_loss = logs[\"loss\"]\n\
          \                    self.final_loss = logs[\"loss\"]\n\n        metrics_callback\
          \ = MetricsCallback(is_main_worker)\n\n        print(\"Downloading and loading\
          \ model\")\n        model_kwargs = {\"device_map\": \"auto\", \"torch_dtype\"\
          : torch.float16}\n        if use_flash_attention:\n            model_kwargs[\"\
          attn_implementation\"] = \"flash_attention_2\"\n\n        model = AutoModelForCausalLM.from_pretrained(model_name,\
          \ **model_kwargs)\n\n        print(f\"Using LoRA target modules for {model_name}:\
          \ {target_modules}\")\n\n        config = LoraConfig(\n            r=lora_rank,\n\
          \            lora_alpha=lora_rank * 2,\n            bias=\"none\",\n   \
          \         lora_dropout=0.05,\n            task_type=\"CAUSAL_LM\",\n   \
          \         target_modules=target_modules,\n        )\n        model = get_peft_model(model,\
          \ config)\n\n        print(\"Loading dataset\")\n        dataset = load_from_disk(dataset_path)\n\
          \n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    \
          \    if tokenizer.pad_token is None:\n            tokenizer.pad_token =\
          \ tokenizer.eos_token\n        tokenizer.padding_side = \"right\"\n\n  \
          \      sft_config = SFTConfig(\n            ## Memory optimization\n   \
          \         gradient_checkpointing=True,\n            gradient_checkpointing_kwargs={\"\
          use_reentrant\": False},\n            gradient_accumulation_steps=1,\n \
          \           per_device_train_batch_size=batch_size,\n            auto_find_batch_size=True,\n\
          \            ## Dataset configuration\n            max_length=max_length,\n\
          \            packing=use_flash_attention,  # Packing works best with Flash\
          \ Attention\n            ## Training parameters\n            num_train_epochs=epochs\
          \ if max_steps is None else None,\n            max_steps=-1 if max_steps\
          \ is None else max_steps,\n            learning_rate=learning_rate,\n  \
          \          optim=optimizer,\n            ## Optimizer parameters\n     \
          \       adam_beta1=adam_beta1,\n            adam_beta2=adam_beta2,\n   \
          \         adam_epsilon=adam_epsilon,\n            weight_decay=weight_decay,\n\
          \            ## Logging and saving\n            logging_steps=logging_steps,\n\
          \            save_steps=save_steps,\n            save_strategy=save_strategy,\n\
          \            logging_dir=\"./logs\",\n            report_to=\"none\",\n\
          \        )\n        trainer = SFTTrainer(\n            model=model,\n  \
          \          processing_class=tokenizer,\n            args=sft_config,\n \
          \           train_dataset=dataset,\n            callbacks=[metrics_callback],\n\
          \        )\n\n        train_result = trainer.train()\n\n        if torch.distributed.is_initialized():\n\
          \            torch.distributed.barrier()\n            print(f\"Worker {world_rank}\
          \ - Training completed and synchronized\")\n\n        if not is_main_worker:\n\
          \            print(\n                f\"Worker {world_rank} - Skipping model\
          \ export and metrics (not main worker)\"\n            )\n            # Clean\
          \ up distributed process group for non-main workers\n            if torch.distributed.is_initialized():\n\
          \                print(f\"Worker {world_rank} - Cleaning up distributed\
          \ process group\")\n                torch.distributed.destroy_process_group()\n\
          \                print(f\"Worker {world_rank} - Distributed process group\
          \ destroyed\")\n            return\n\n        print(\"Main worker (rank\
          \ 0) - Exporting model and metrics...\")\n\n        # Save LoRA adapter\n\
          \        model_output_path = os.path.join(pvc_path, \"adapter\")\n     \
          \   model.save_pretrained(model_output_path)\n        tokenizer.save_pretrained(model_output_path)\n\
          \        print(\"LoRA adapter exported successfully!\")\n\n        # Merge\
          \ LoRA adapter with base model and save merged model for evaluation\n  \
          \      if save_merged_model_path:\n            print(\n                f\"\
          Merging LoRA adapter with base model and saving to {save_merged_model_path}...\"\
          \n            )\n            merged_model = model.merge_and_unload()\n \
          \           merged_model.save_pretrained(save_merged_model_path)\n     \
          \       tokenizer.save_pretrained(save_merged_model_path)\n            print(f\"\
          Merged model saved to {save_merged_model_path}\")\n\n        # Clean up\
          \ distributed process group for main worker AFTER model saving\n       \
          \ if torch.distributed.is_initialized():\n            print(f\"Worker {world_rank}\
          \ - Cleaning up distributed process group\")\n            torch.distributed.destroy_process_group()\n\
          \            print(f\"Worker {world_rank} - Distributed process group destroyed\"\
          )\n\n        print(f\"Collecting essential metrics\")\n        metrics_dict\
          \ = {}\n\n        if hasattr(train_result, \"train_loss\"):\n          \
          \  metrics_dict[\"final_train_loss\"] = train_result.train_loss\n      \
          \  if hasattr(train_result, \"train_runtime\"):\n            metrics_dict[\"\
          train_runtime_seconds\"] = train_result.train_runtime\n        if hasattr(train_result,\
          \ \"train_samples_per_second\"):\n            metrics_dict[\"throughput_samples_per_sec\"\
          ] = (\n                train_result.train_samples_per_second\n         \
          \   )\n\n        total_params = sum(p.numel() for p in model.parameters())\n\
          \        trainable_params = sum(p.numel() for p in model.parameters() if\
          \ p.requires_grad)\n        metrics_dict[\"total_parameters_millions\"]\
          \ = total_params / 1_000_000\n        metrics_dict[\"trainable_parameters_millions\"\
          ] = trainable_params / 1_000_000\n        metrics_dict[\"lora_efficiency_percent\"\
          ] = (\n            trainable_params / total_params\n        ) * 100\n\n\
          \        metrics_dict[\"lora_rank\"] = config.r\n        metrics_dict[\"\
          learning_rate\"] = sft_config.learning_rate\n        metrics_dict[\"effective_batch_size\"\
          ] = (\n            sft_config.per_device_train_batch_size * world_size\n\
          \        )\n        metrics_dict[\"dataset_size\"] = len(dataset)\n\n  \
          \      metrics_dict[\"num_nodes\"] = (\n            world_size // torch.cuda.device_count()\n\
          \            if torch.cuda.is_available() and torch.cuda.device_count()\
          \ > 0\n            else 1\n        )\n        if torch.cuda.is_available():\n\
          \            metrics_dict[\"peak_gpu_memory_gb\"] = torch.cuda.max_memory_allocated()\
          \ / (\n                1024**3\n            )\n\n        if metrics_callback.initial_loss\
          \ and metrics_callback.final_loss:\n            metrics_dict[\"initial_loss\"\
          ] = metrics_callback.initial_loss\n            metrics_dict[\"loss_reduction\"\
          ] = (\n                metrics_callback.initial_loss - metrics_callback.final_loss\n\
          \            )\n            metrics_dict[\"loss_reduction_percent\"] = (\n\
          \                (metrics_callback.initial_loss - metrics_callback.final_loss)\n\
          \                / metrics_callback.initial_loss\n            ) * 100\n\n\
          \        with open(os.path.join(pvc_path, \"metrics.json\"), \"w\") as f:\n\
          \            json.dump(metrics_dict, f, indent=2)\n\n        print(\n  \
          \          f\"Exported {len(metrics_dict)} metrics to {os.path.join(pvc_path,\
          \ 'metrics.json')}\"\n        )\n        print(\"Model and metrics exported\
          \ successfully!\")\n\n    print(\"=== Starting TrainJob creation process\
          \ ===\")\n\n    target_modules = get_target_modules(model_name)\n    print(f\"\
          Selected LoRA target modules for {model_name}: {target_modules}\")\n\n \
          \   with open(\n        \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\
          , \"r\"\n    ) as ns_file:\n        namespace = ns_file.readline()\n\n \
          \   print(\"Generating command...\")\n\n    func_code = inspect.getsource(train_model_func)\n\
          \    func_code = textwrap.dedent(func_code)\n\n    func_call_code = f\"\"\
          \"\nimport os\nimport json\n\n# Parse function arguments from environment\
          \ variable\nconfig_json = os.environ.get(\"TRAINING_CONFIG\", \"{{}}\")\n\
          func_args = json.loads(config_json)\n\n# Call the training function with\
          \ parsed arguments\n{train_model_func.__name__}(**func_args)\n\"\"\"\n\n\
          \    func_code = f\"{func_code}\\n{func_call_code}\"\n\n    # Build package\
          \ list based on configuration\n    packages = [\"transformers\", \"peft\"\
          , \"accelerate\", \"trl\"]\n    if use_flash_attention:\n        packages.append(\"\
          flash-attn\")\n    packages_str = \" \".join(packages)\n\n    install_script\
          \ = f\"\"\"set -e\nset -o pipefail\n\necho \"=== Starting container setup\
          \ ===\"\necho \"Python version: $(python --version)\"\n\nif ! [ -x \"$(command\
          \ -v pip)\" ]; then\n    echo \"Installing pip...\"\n    python -m ensurepip\
          \ || python -m ensurepip --user\nfi\n\necho \"Installing Python packages...\"\
          \nPIP_DISABLE_PIP_VERSION_CHECK=1 python -m pip install --user --quiet --no-warn-script-location\
          \ {packages_str}\n\necho \"Creating training script...\"\ncat > ephemeral_component.py\
          \ << 'EOF'\n{func_code}\nEOF\n\necho \"Starting distributed training...\"\
          \ntorchrun ephemeral_component.py\"\"\"\n\n    command = [\"bash\", \"-c\"\
          , install_script]\n\n    print(f\"Generated command: {command}\")\n    print(f\"\
          Command length: {len(command)}\")\n    print(f\"Command type: {type(command)}\"\
          )\n\n    print(\"Loading Kubernetes configuration...\")\n    try:\n    \
          \    config.load_incluster_config()\n        print(\"Loaded in-cluster Kubernetes\
          \ configuration\")\n    except config.ConfigException:\n        config.load_kube_config()\n\
          \        print(\"Loaded kubeconfig Kubernetes configuration\")\n\n    print(\"\
          Creating Kubernetes API client...\")\n    api_client = k8s_client.ApiClient()\n\
          \    custom_objects_api = k8s_client.CustomObjectsApi(api_client)\n    print(\"\
          Successfully created Kubernetes API client\")\n\n    print(\"Defining TrainJob\
          \ resource...\")\n    train_job = {\n        \"apiVersion\": \"trainer.kubeflow.org/v1alpha1\"\
          ,\n        \"kind\": \"TrainJob\",\n        \"metadata\": {\"name\": f\"\
          kfp-{run_id}\", \"namespace\": namespace},\n        \"spec\": {\n      \
          \      \"runtimeRef\": {\"name\": trainer_runtime},\n            \"trainer\"\
          : {\n                \"numNodes\": num_nodes,\n                \"resourcesPerNode\"\
          : {\n                    \"requests\": {\n                        \"cpu\"\
          : train_node_cpu_request,\n                        \"memory\": train_node_memory_request,\n\
          \                        \"nvidia.com/gpu\": train_node_gpu_request,\n \
          \                   },\n                    \"limits\": {\n            \
          \            \"cpu\": train_node_cpu_request,\n                        \"\
          memory\": train_node_memory_request,\n                        \"nvidia.com/gpu\"\
          : train_node_gpu_request,\n                    },\n                },\n\
          \                \"env\": [\n                    {\"name\": \"HOME\", \"\
          value\": \"/tmp\"},\n                    {\n                        \"name\"\
          : \"TRAINING_CONFIG\",\n                        \"value\": json.dumps(\n\
          \                            {\n                                \"lora_rank\"\
          : lora_rank,\n                                \"learning_rate\": learning_rate,\n\
          \                                \"batch_size\": batch_size,\n         \
          \                       \"max_length\": max_length,\n                  \
          \              \"model_name\": model_name,\n                           \
          \     \"dataset_path\": dataset_path,\n                                \"\
          epochs\": epochs,\n                                \"pvc_path\": pvc_path,\n\
          \                                \"target_modules\": target_modules,\n \
          \                               \"max_steps\": max_steps,\n            \
          \                    \"logging_steps\": logging_steps,\n               \
          \                 \"save_steps\": save_steps,\n                        \
          \        \"save_strategy\": save_strategy,\n                           \
          \     \"optimizer\": optimizer,\n                                \"adam_beta1\"\
          : adam_beta1,\n                                \"adam_beta2\": adam_beta2,\n\
          \                                \"adam_epsilon\": adam_epsilon,\n     \
          \                           \"weight_decay\": weight_decay,\n          \
          \                      \"use_flash_attention\": use_flash_attention,\n \
          \                               \"save_merged_model_path\": save_merged_model_path,\n\
          \                            }\n                        ),\n           \
          \         },\n                ],\n                \"command\": command,\n\
          \            },\n            \"podSpecOverrides\": [\n                {\n\
          \                    \"targetJobs\": [{\"name\": \"node\"}],\n         \
          \           \"volumes\": [\n                        {\n                \
          \            \"name\": \"dataset-pvc\",\n                            \"\
          persistentVolumeClaim\": {\"claimName\": pvc_name},\n                  \
          \      }\n                    ],\n                    \"containers\": [\n\
          \                        {\n                            \"name\": \"node\"\
          ,\n                            \"volumeMounts\": [\n                   \
          \             {\"name\": \"dataset-pvc\", \"mountPath\": \"/workspace\"\
          }\n                            ],\n                        }\n         \
          \           ],\n                }\n            ],\n        },\n    }\n\n\
          \    print(f\"TrainJob definition created:\")\n    print(f\"  - Name: kfp-{run_id}\"\
          )\n    print(f\"  - Namespace: {namespace}\")\n\n    print(f\"  - Runtime:\
          \ {trainer_runtime}\")\n    print(f\"  - Nodes: {num_nodes}\")\n    print(f\"\
          \  - PVC: {pvc_name}\")\n    print(f\"  - Model: {model_name}\")\n    print(f\"\
          \  - Dataset: {dataset_path}\")\n    print(f\"  - Epochs: {epochs}\")\n\n\
          \    print(\"Submitting TrainJob to Kubernetes...\")\n    try:\n       \
          \ response = custom_objects_api.create_namespaced_custom_object(\n     \
          \       group=\"trainer.kubeflow.org\",\n            version=\"v1alpha1\"\
          ,\n            namespace=namespace,\n            plural=\"trainjobs\",\n\
          \            body=train_job,\n        )\n        job_name = response[\"\
          metadata\"][\"name\"]\n        print(f\"TrainJob {job_name} created successfully\"\
          )\n        print(f\"Response metadata: {response.get('metadata', {})}\"\
          )\n    except ApiException as e:\n        print(f\"Error creating TrainJob:\
          \ {e}\")\n        print(f\"Error details: {e.body}\")\n        print(f\"\
          Error status: {e.status}\")\n        raise\n\n    print(f\"Starting to monitor\
          \ TrainJob {job_name} status...\")\n    check_count = 0\n    while True:\n\
          \        check_count += 1\n        try:\n            print(f\"Checking job\
          \ status (attempt {check_count})...\")\n            job_status = custom_objects_api.get_namespaced_custom_object(\n\
          \                group=\"trainer.kubeflow.org\",\n                version=\"\
          v1alpha1\",\n                namespace=namespace,\n                plural=\"\
          trainjobs\",\n                name=job_name,\n            )\n\n        \
          \    status = job_status.get(\"status\", {})\n            conditions = status.get(\"\
          conditions\", [])\n            print(f\"Job status conditions: {conditions}\"\
          )\n\n            completed = False\n            failed = False\n\n     \
          \       for condition in conditions:\n                condition_type = condition.get(\"\
          type\", \"\")\n                condition_status = condition.get(\"status\"\
          , \"\")\n                condition_reason = condition.get(\"reason\", \"\
          \")\n                condition_message = condition.get(\"message\", \"\"\
          )\n\n                print(\n                    f\"Condition: type={condition_type},\
          \ status={condition_status}, reason={condition_reason}\"\n             \
          \   )\n\n                if condition_type == \"Complete\" and condition_status\
          \ == \"True\":\n                    print(\n                        f\"\
          Training job {job_name} completed successfully: {condition_message}\"\n\
          \                    )\n                    completed = True\n         \
          \           break\n                elif condition_type == \"Failed\" and\
          \ condition_status == \"True\":\n                    print(f\"Training job\
          \ {job_name} failed: {condition_message}\")\n                    failed\
          \ = True\n                    break\n                elif condition_type\
          \ == \"Cancelled\" and condition_status == \"True\":\n                 \
          \   print(f\"Training job {job_name} was cancelled: {condition_message}\"\
          )\n                    failed = True\n                    break\n\n    \
          \        if completed:\n                break\n            elif failed:\n\
          \                raise RuntimeError(f\"Training job {job_name} failed or\
          \ was cancelled\")\n            else:\n                print(f\"Job is still\
          \ running, continuing to wait...\")\n\n        except ApiException as e:\n\
          \            print(f\"Error checking job status: {e}\")\n            print(f\"\
          Error details: {e.body}\")\n\n        print(f\"Waiting 10 seconds before\
          \ next check...\")\n        time.sleep(10)\n\n    print(f\"Training job\
          \ {job_name} completed. Logs would be retrieved here.\")\n\n    print(\"\
          Processing training results...\")\n\n    metrics_file_path = os.path.join(pvc_path,\
          \ \"metrics.json\")\n    print(f\"Looking for metrics file at: {metrics_file_path}\"\
          )\n    if os.path.exists(metrics_file_path):\n        print(f\"Found metrics\
          \ file, reading from {metrics_file_path}\")\n        with open(metrics_file_path,\
          \ \"r\") as f:\n            metrics_dict = json.load(f)\n\n        print(f\"\
          Loaded {len(metrics_dict)} metrics from file\")\n\n        exported_count\
          \ = 0\n        for metric_name, metric_value in metrics_dict.items():\n\
          \            # Ignore metrics that are 0 to avoid a bug in the RHOAI UI.\n\
          \            if isinstance(metric_value, (int, float)) and metric_value\
          \ != 0:\n                output_metrics.log_metric(metric_name, metric_value)\n\
          \                print(f\"Exported metric: {metric_name} = {metric_value}\"\
          )\n                exported_count += 1\n\n        print(f\"Successfully\
          \ exported {exported_count} metrics to Kubeflow\")\n        os.remove(metrics_file_path)\n\
          \    else:\n        print(f\"Warning: Metrics file {metrics_file_path} not\
          \ found\")\n\n    print(\"Copying model from PVC to Kubeflow output path...\"\
          )\n    model_source = os.path.join(pvc_path, \"adapter\")\n    print(f\"\
          Model source: {model_source}\")\n    print(f\"Destination: {output_model.path}\"\
          )\n\n    if not os.path.exists(model_source):\n        raise FileNotFoundError(\n\
          \            f\"Trained model not found at expected location: {model_source}\"\
          \n        )\n\n    output_model.name = f\"{model_name}-adapter\"\n    shutil.copytree(model_source,\
          \ output_model.path, dirs_exist_ok=True)\n    print(f\"Model copied successfully\
          \ from {model_source} to {output_model.path}\")\n\n    print(\"=== TrainJob\
          \ process completed successfully ===\")\n\n"
        image: registry.access.redhat.com/ubi9/python-311:latest
pipelineInfo:
  name: train-model
root:
  dag:
    outputs:
      artifacts:
        output_metrics:
          artifactSelectors:
          - outputArtifactKey: output_metrics
            producerSubtask: train-model
        output_model:
          artifactSelectors:
          - outputArtifactKey: output_model
            producerSubtask: train-model
    tasks:
      train-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-model
        inputs:
          parameters:
            adam_beta1:
              componentInputParameter: adam_beta1
            adam_beta2:
              componentInputParameter: adam_beta2
            adam_epsilon:
              componentInputParameter: adam_epsilon
            batch_size:
              componentInputParameter: batch_size
            dataset_path:
              componentInputParameter: dataset_path
            epochs:
              componentInputParameter: epochs
            learning_rate:
              componentInputParameter: learning_rate
            logging_steps:
              componentInputParameter: logging_steps
            lora_rank:
              componentInputParameter: lora_rank
            max_length:
              componentInputParameter: max_length
            max_steps:
              componentInputParameter: max_steps
            model_name:
              componentInputParameter: model_name
            num_nodes:
              componentInputParameter: num_nodes
            optimizer:
              componentInputParameter: optimizer
            pvc_name:
              componentInputParameter: pvc_name
            pvc_path:
              componentInputParameter: pvc_path
            run_id:
              componentInputParameter: run_id
            save_merged_model_path:
              componentInputParameter: save_merged_model_path
            save_steps:
              componentInputParameter: save_steps
            save_strategy:
              componentInputParameter: save_strategy
            train_node_cpu_request:
              componentInputParameter: train_node_cpu_request
            train_node_gpu_request:
              componentInputParameter: train_node_gpu_request
            train_node_memory_request:
              componentInputParameter: train_node_memory_request
            trainer_runtime:
              componentInputParameter: trainer_runtime
            use_flash_attention:
              componentInputParameter: use_flash_attention
            weight_decay:
              componentInputParameter: weight_decay
        taskInfo:
          name: train-model
  inputDefinitions:
    parameters:
      adam_beta1:
        defaultValue: 0.9
        description: Beta1 parameter for Adam optimizer. Defaults to 0.9.
        isOptional: true
        parameterType: NUMBER_DOUBLE
      adam_beta2:
        defaultValue: 0.999
        description: Beta2 parameter for Adam optimizer. Defaults to 0.999.
        isOptional: true
        parameterType: NUMBER_DOUBLE
      adam_epsilon:
        defaultValue: 1.0e-08
        description: Epsilon parameter for Adam optimizer. Defaults to 1e-8.
        isOptional: true
        parameterType: NUMBER_DOUBLE
      batch_size:
        defaultValue: 16.0
        description: Per-device training batch size. Defaults to 16.
        isOptional: true
        parameterType: NUMBER_INTEGER
      dataset_path:
        description: Path to the training dataset within the PVC.
        parameterType: STRING
      epochs:
        defaultValue: 10.0
        description: Number of training epochs. Defaults to 10.
        isOptional: true
        parameterType: NUMBER_INTEGER
      learning_rate:
        defaultValue: 0.0003
        description: Learning rate for training optimization. Defaults to 3e-4.
        isOptional: true
        parameterType: NUMBER_DOUBLE
      logging_steps:
        defaultValue: 10.0
        description: Number of steps between logging outputs. Defaults to 10.
        isOptional: true
        parameterType: NUMBER_INTEGER
      lora_rank:
        defaultValue: 8.0
        description: LoRA adapter rank (lower = fewer parameters, faster training).
          Defaults to 8.
        isOptional: true
        parameterType: NUMBER_INTEGER
      max_length:
        defaultValue: 64.0
        description: Maximum token sequence length for training. Defaults to 64.
        isOptional: true
        parameterType: NUMBER_INTEGER
      max_steps:
        description: Maximum number of training steps. If specified, overrides epochs.
          Defaults to None.
        isOptional: true
        parameterType: NUMBER_INTEGER
      model_name:
        description: HuggingFace model identifier (e.g., "ibm-granite/granite-3.3-8b-instruct").
        parameterType: STRING
      num_nodes:
        defaultValue: 2.0
        description: Number of nodes for distributed training. Defaults to 2.
        isOptional: true
        parameterType: NUMBER_INTEGER
      optimizer:
        defaultValue: adamw_torch
        description: Optimizer to use (e.g., "adamw_torch", "adamw_torch_fused").
          Defaults to "adamw_torch".
        isOptional: true
        parameterType: STRING
      pvc_name:
        description: Name of the Persistent Volume Claim for data storage that is
          mounted to this component.
        parameterType: STRING
      pvc_path:
        description: Base path within the PVC for storing outputs.
        parameterType: STRING
      run_id:
        description: Unique identifier for this training run. Use dsl.PIPELINE_JOB_ID_PLACEHOLDER.
        parameterType: STRING
      save_merged_model_path:
        description: Path to save the merged model (base + LoRA adapter). Useful for
          saving to the PVC for evaluation. Defaults to None.
        isOptional: true
        parameterType: STRING
      save_steps:
        description: Number of steps between model checkpoints. Defaults to None.
        isOptional: true
        parameterType: NUMBER_INTEGER
      save_strategy:
        defaultValue: epoch
        description: Checkpoint saving strategy ("epoch" or "steps"). Defaults to
          "epoch".
        isOptional: true
        parameterType: STRING
      train_node_cpu_request:
        defaultValue: '2'
        description: CPU request per node (e.g., "2", "4"). Defaults to "2".
        isOptional: true
        parameterType: STRING
      train_node_gpu_request:
        defaultValue: '1'
        description: GPU request per node (e.g., "1", "2"). Defaults to "1".
        isOptional: true
        parameterType: STRING
      train_node_memory_request:
        defaultValue: 100Gi
        description: Memory request per node (e.g., "100Gi", "200Gi"). Defaults to
          "100Gi".
        isOptional: true
        parameterType: STRING
      trainer_runtime:
        defaultValue: torch-distributed
        description: Runtime to use for Kubeflow Trainer. Defaults to "torch-distributed".
        isOptional: true
        parameterType: STRING
      use_flash_attention:
        defaultValue: false
        description: Whether to use Flash Attention 2 for improved performance. Defaults
          to False.
        isOptional: true
        parameterType: BOOLEAN
      weight_decay:
        defaultValue: 0.01
        description: Weight decay for regularization. Defaults to 0.01.
        isOptional: true
        parameterType: NUMBER_DOUBLE
  outputDefinitions:
    artifacts:
      output_metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      output_model:
        artifactType:
          schemaTitle: system.Model
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.14.1
